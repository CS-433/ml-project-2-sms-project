{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sorted-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "yellow-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabulous-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "exercises_dict = {\n",
    "    'Abduction': 0,\n",
    "    'Bird': 1,\n",
    "    'Bridge': 2,\n",
    "    'Knee': 3,\n",
    "    'Shoulder': 4,\n",
    "    'Squat' : 5,\n",
    "    'Stretch' : 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "operational-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet('dataset.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f294cf20-6674-47f0-a3f8-2c2df32d5765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2210290, 104)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "attached-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()\n",
    "#size = 100000\n",
    "#dataset = dataset[:size]\n",
    "data = dataset.values\n",
    "indices = dataset.index\n",
    "labels = dataset.columns\n",
    "\n",
    "x_train = data[:,4:]\n",
    "y_train = np.array([exercises_dict[e] for e in data[:,1]])#[::4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "spoken-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a25121ed-b880-4c2c-a328-1d77bca37d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2183099, 104)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dutch-ecuador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Participant' 'P04']\n",
      " ['Exercise' 'Abduction']\n",
      " ['Set' 'A']\n",
      " ['Camera' 'Frontal_Top']\n",
      " ['time(s)' 0.733]\n",
      " ['left_ankle_x' 0.013897059485316277]\n",
      " ['left_ankle_y' 0.7337374687194824]\n",
      " ['left_ankle_z' 0.3085203766822815]\n",
      " ['left_ear_x' -0.004014772828668356]\n",
      " ['left_ear_y' -0.6559332609176636]\n",
      " ['left_ear_z' 0.013528380542993546]\n",
      " ['left_elbow_x' 0.2160586714744568]\n",
      " ['left_elbow_y' -0.45321035385131836]\n",
      " ['left_elbow_z' 0.20046469569206238]\n",
      " ['left_eye_center_x' 0.0743502825498581]\n",
      " ['left_eye_center_y' -0.6516526341438293]\n",
      " ['left_eye_center_z' -0.07846864312887192]\n",
      " ['left_eye_inner_x' 0.07402841001749039]\n",
      " ['left_eye_inner_y' -0.6512871980667114]\n",
      " ['left_eye_inner_z' -0.07923341542482376]\n",
      " ['left_eye_outer_x' 0.07386316359043121]\n",
      " ['left_eye_outer_y' -0.6522524952888489]\n",
      " ['left_eye_outer_z' -0.07846874743700027]\n",
      " ['left_foot_x' 0.12216062843799591]\n",
      " ['left_foot_y' 0.8188711404800415]\n",
      " ['left_foot_z' 0.2560451626777649]\n",
      " ['left_heel_x' 0.024564087390899658]\n",
      " ['left_heel_y' 0.7773051857948303]\n",
      " ['left_heel_z' 0.3135857880115509]\n",
      " ['left_hip_x' 0.049113743007183075]\n",
      " ['left_hip_y' 0.0017500214744359255]\n",
      " ['left_hip_z' 0.10653668642044067]\n",
      " ['left_index_x' 0.4910612404346466]\n",
      " ['left_index_y' -0.5773735046386719]\n",
      " ['left_index_z' 0.10218209028244019]\n",
      " ['left_knee_x' 0.00890833605080843]\n",
      " ['left_knee_y' 0.4022276997566223]\n",
      " ['left_knee_z' 0.19068723917007446]\n",
      " ['left_mouth_x' 0.0655469000339508]\n",
      " ['left_mouth_y' -0.5923643112182617]\n",
      " ['left_mouth_z' -0.0553886741399765]\n",
      " ['left_pinky_x' 0.48987892270088196]\n",
      " ['left_pinky_y' -0.5560035109519958]\n",
      " ['left_pinky_z' 0.13134904205799103]\n",
      " ['left_shoulder_x' 0.01879825070500374]\n",
      " ['left_shoulder_y' -0.4777016043663025]\n",
      " ['left_shoulder_z' 0.14446546137332916]\n",
      " ['left_thumb_x' 0.465911328792572]\n",
      " ['left_thumb_y' -0.5344457626342773]\n",
      " ['left_thumb_z' 0.11610101163387299]\n",
      " ['left_wrist_x' 0.4539792537689209]\n",
      " ['left_wrist_y' -0.5236285924911499]\n",
      " ['left_wrist_z' 0.13143183290958405]\n",
      " ['nose_x' 0.09227589517831802]\n",
      " ['nose_y' -0.6114740371704102]\n",
      " ['nose_z' -0.09044832736253738]\n",
      " ['right_ankle_x' -0.003346616867929697]\n",
      " ['right_ankle_y' 0.7445031404495239]\n",
      " ['right_ankle_z' 0.013860464096069336]\n",
      " ['right_ear_x' -0.05203988030552864]\n",
      " ['right_ear_y' -0.6311370730400085]\n",
      " ['right_ear_z' -0.12460330128669739]\n",
      " ['right_elbow_x' 0.18054381012916565]\n",
      " ['right_elbow_y' -0.44995322823524475]\n",
      " ['right_elbow_z' -0.2860133647918701]\n",
      " ['right_eye_center_x' 0.06493184715509415]\n",
      " ['right_eye_center_y' -0.6470574736595154]\n",
      " ['right_eye_center_z' -0.10832128673791885]\n",
      " ['right_eye_inner_x' 0.06424965709447861]\n",
      " ['right_eye_inner_y' -0.6460078358650208]\n",
      " ['right_eye_inner_z' -0.10767758637666702]\n",
      " ['right_eye_outer_x' 0.06447746604681015]\n",
      " ['right_eye_outer_y' -0.6482470631599426]\n",
      " ['right_eye_outer_z' -0.10804957896471024]\n",
      " ['right_foot_x' 0.11092636734247208]\n",
      " ['right_foot_y' 0.8169887661933899]\n",
      " ['right_foot_z' -0.0854569673538208]\n",
      " ['right_heel_x' 0.0032588159665465355]\n",
      " ['right_heel_y' 0.789229691028595]\n",
      " ['right_heel_z' 0.013374772854149342]\n",
      " ['right_hip_x' -0.05006172135472298]\n",
      " ['right_hip_y' -0.0021967727225273848]\n",
      " ['right_hip_z' -0.10514145344495773]\n",
      " ['right_index_x' 0.47694534063339233]\n",
      " ['right_index_y' -0.5742470622062683]\n",
      " ['right_index_z' -0.30002376437187195]\n",
      " ['right_knee_x' -0.04012124985456467]\n",
      " ['right_knee_y' 0.3869994580745697]\n",
      " ['right_knee_z' -0.050813380628824234]\n",
      " ['right_mouth_x' 0.051128536462783813]\n",
      " ['right_mouth_y' -0.5850277543067932]\n",
      " ['right_mouth_z' -0.09606275707483292]\n",
      " ['right_pinky_x' 0.48278337717056274]\n",
      " ['right_pinky_y' -0.5279112458229065]\n",
      " ['right_pinky_z' -0.3223920464515686]\n",
      " ['right_shoulder_x' -0.0720675140619278]\n",
      " ['right_shoulder_y' -0.4870046377182007]\n",
      " ['right_shoulder_z' -0.222687765955925]\n",
      " ['right_thumb_x' 0.4476351737976074]\n",
      " ['right_thumb_y' -0.5260311365127563]\n",
      " ['right_thumb_z' -0.2789108157157898]\n",
      " ['right_wrist_x' 0.4336269497871399]\n",
      " ['right_wrist_y' -0.5065780878067017]\n",
      " ['right_wrist_z' -0.2859187126159668]]\n"
     ]
    }
   ],
   "source": [
    "#example of a row with labels\n",
    "print(np.stack((labels.to_numpy(),data[4 * 1793 + 1]), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handy-horizon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKING:\n",
      "(552545, 100)\n",
      "(552545, 100)\n",
      "(552545, 100)\n",
      "(552545, 100)\n"
     ]
    }
   ],
   "source": [
    "#Splits the dataset in a 4-channel data set (think of an RGB Image) coresponding to the 4 different cameras\n",
    "\n",
    "lengths = np.append(np.where(indices.array[:len(indices)] == 0)[0], [len(indices)])\n",
    "ft_x, fl_x, st_x, sl_x = [], [], [], []\n",
    "mod4 = 0\n",
    "gap = 0\n",
    "\n",
    "for idx, l in enumerate(lengths):\n",
    "    if idx < len(lengths)-1:\n",
    "        end = lengths[idx + 1]\n",
    "        \n",
    "        if mod4 == 0:\n",
    "            gap = end - l\n",
    "\n",
    "        segment = x_train[l:l + gap]\n",
    "        \n",
    "        if mod4 == 0:\n",
    "            ft_x.append(segment)\n",
    "        elif mod4 == 1:\n",
    "            fl_x.append(segment)\n",
    "\n",
    "        elif mod4 == 2:\n",
    "            st_x.append(segment)\n",
    "\n",
    "        elif mod4 == 3:\n",
    "            sl_x.append(segment)\n",
    "        \n",
    "        mod4 = (mod4 + 1) % 4\n",
    "\n",
    "print(\"STACKING:\")\n",
    "# Concatenate the lists into arrays\n",
    "ft_x = np.vstack(ft_x) \n",
    "fl_x = np.vstack(fl_x)\n",
    "st_x = np.vstack(st_x) \n",
    "sl_x = np.vstack(sl_x)\n",
    "\n",
    "x_train = np.array([ft_x, fl_x, st_x, sl_x])\n",
    "print(ft_x.shape)\n",
    "print(fl_x.shape)\n",
    "print(st_x.shape)\n",
    "print(sl_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "printable-simon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(552545, 100, 4)\n",
      "(2183099,)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.moveaxis(x_train,-1,0)\n",
    "x_train = np.moveaxis(x_train,-1,0)\n",
    "y_train = y_train#[:552545]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lined-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mlp = lambda: torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(400, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 7),\n",
    ")\n",
    "\n",
    "get_cnn = lambda: torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(in_channels=4, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv1d(32, 64, 5, stride=2, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv1d(64, 64, 5, stride=1, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv1d(64, 128, 5, stride=2, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.AdaptiveAvgPool1d(1),\n",
    "    torch.nn.Conv1d(128, 7, 1),\n",
    "    torch.nn.Flatten(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "laden-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Return a modified dataset cls that can insert MNIST like images into larger\n",
    "frames with an option for random shifts, scrambling the images in a\n",
    "consistent way (using the same shuffling for all images) and adding random\n",
    "Gaussian noise (to the base data, noise is always the same for a given\n",
    "image).\n",
    "\"\"\"\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_train, y_train, transform=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'input': self.x_train[idx], 'label': self.y_train[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "suffering-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 400  # Number of input channels\n",
    "hidden_size = 64  # Size of the hidden layer\n",
    "output_size = 7  # Number of classes (0 to 6)\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "custom_dataset = CustomDataset(x_train, y_train)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 64\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = get_mlp()\n",
    "model_path = 'MLP.path'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "armed-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1326\n",
      "Epoch [2/10], Loss: 0.1248\n",
      "Epoch [3/10], Loss: 0.1189\n",
      "Epoch [4/10], Loss: 0.1137\n",
      "Epoch [5/10], Loss: 0.1089\n",
      "Epoch [6/10], Loss: 0.1058\n",
      "Epoch [7/10], Loss: 0.1027\n",
      "Epoch [8/10], Loss: 0.0990\n",
      "Epoch [9/10], Loss: 0.0971\n",
      "Epoch [10/10], Loss: 0.0959\n"
     ]
    }
   ],
   "source": [
    "#def train_epoch()\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch in custom_dataloader:\n",
    "        inputs, labels = batch['input'], batch['label']\n",
    "\n",
    "        # Flatten the inputs (assuming the data is in the shape (batch_size, 100, 4))\n",
    "        inputs = inputs.view(-1, input_size)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {np.mean(losses):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "european-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'MLP.path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "moved-brisbane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4, 552545)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.moveaxis(x_train,-1,0)\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "expanded-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 400  # Number of input channels\n",
    "hidden_size = 64  # Size of the hidden layer\n",
    "output_size = 7  # Number of classes (0 to 6)\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "custom_dataset = CustomDataset(x_train, y_train)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 64\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = get_cnn()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch in custom_dataloader:\n",
    "        inputs, labels = batch['input'], batch['label']\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(loss)\n",
    "        losses.append(loss.item())\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {np.mean(losses):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-distribution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
