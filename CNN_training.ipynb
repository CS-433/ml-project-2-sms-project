{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yellow-wilderness"
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
    "from torch.optim import lr_scheduler\n",
    "import pandas as pd"
   ],
   "id": "yellow-wilderness"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2385,
     "status": "ok",
     "timestamp": 1703117726686,
     "user": {
      "displayName": "baptiste maquignaz",
      "userId": "09858058211747966059"
     },
     "user_tz": -60
    },
    "id": "nGGne-3u1TSE",
    "outputId": "db5b1eed-28e4-408e-e971-897d802c805c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ],
   "id": "nGGne-3u1TSE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "operational-depth"
   },
   "outputs": [],
   "source": [
    "#Change paths\n",
    "train_path = 'train_set_augmented.parquet'\n",
    "test_path = 'test_set.parquet'\n",
    "\n",
    "train_df = pd.read_parquet(train_path, engine='pyarrow')\n",
    "test_df = pd.read_parquet(test_path, engine='pyarrow')"
   ],
   "id": "operational-depth"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ghh0TVI9EVP"
   },
   "outputs": [],
   "source": [
    "BLOCK_LENGTH = 1\n",
    "test_video_indices = test_df.groupby('video_id').size().values\n",
    "test_video_indices = np.insert(test_video_indices, 0, 0)\n",
    "test_video_indices = np.cumsum(test_video_indices)\n",
    "\n",
    "train_video_indices = train_df.groupby('video_id').size().values\n",
    "train_video_indices = np.insert(train_video_indices, 0, 0)\n",
    "train_video_indices = np.cumsum(train_video_indices)\n",
    "\n",
    "set_mapping = {\n",
    "    'Correct' : 0,\n",
    "    'A': 1,\n",
    "    'B': 2,\n",
    "    'C': 3,\n",
    "    'D': 4,\n",
    "    'E': 5,\n",
    "    'F': 6\n",
    "}"
   ],
   "id": "_Ghh0TVI9EVP"
  },
  {
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = helpers.import_data_set(train_df, test_df, set_mapping, True)"
   ],
   "metadata": {
    "id": "SUfcNhePBHG9"
   },
   "id": "SUfcNhePBHG9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train = helpers.pad_sequences(X_train, BLOCK_LENGTH, train_video_indices)\n",
    "X_train = X_train.reshape(-1,1,BLOCK_LENGTH,107)\n",
    "X_test = helpers.pad_sequences(X_test, BLOCK_LENGTH, test_video_indices)\n",
    "X_test = X_test.reshape(-1,1,BLOCK_LENGTH,107)\n",
    "y_train = helpers.pad_sequences(y_train, BLOCK_LENGTH, train_video_indices)\n",
    "y_train = y_train[::BLOCK_LENGTH].squeeze()\n",
    "y_test = helpers.pad_sequences(y_test, BLOCK_LENGTH, test_video_indices)\n",
    "y_test = y_test[::BLOCK_LENGTH].squeeze()"
   ],
   "metadata": {
    "id": "6Sz0gTxYAvEr"
   },
   "id": "6Sz0gTxYAvEr",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "video_indices = test_df.groupby('video_id').size().values\n",
    "video_indices = np.ceil(video_indices / BLOCK_LENGTH)\n",
    "video_indices = np.insert(video_indices, 0, 0)\n",
    "video_indices = np.cumsum(video_indices).astype(int)"
   ],
   "metadata": {
    "id": "4lI0a7JrLB2o"
   },
   "id": "4lI0a7JrLB2o",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define the CNN models as follows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "get_cnn2b = lambda: torch.nn.Sequential(\n",
    "    # Block 1\n",
    "    nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # Block 2\n",
    "    nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(832 * BLOCK_LENGTH, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 7)\n",
    "    )"
   ],
   "metadata": {
    "id": "2egtCevFgwV3"
   },
   "id": "2egtCevFgwV3",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "get_cnn3b = lambda: torch.nn.Sequential(\n",
    "    # Block 1\n",
    "    nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # Block 2\n",
    "    nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # Block 3\n",
    "    nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # Fully connected layers\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(53248, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(4096, 256),  # Adjust the output size based on your task\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(256, 7),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ],
   "metadata": {
    "id": "z2NyZ1gAsHLZ"
   },
   "id": "z2NyZ1gAsHLZ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the model, change this if you want to test cnn3b\n",
    "model = get_cnn2b()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIQsffXVrQFy"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Running on {device}\")\n",
    "# Create an instance of the custom dataset\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model_path = ''\n",
    "model.load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))\n",
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n"
   ],
   "id": "uIQsffXVrQFy"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oZ6yVkTiDLa",
    "outputId": "284e8d3c-562d-4b09-a7f3-ae8715256b9d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing: 64.25%"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "  helpers.train_model(1, model, optimizer, criterion, dataloader, scheduler, model_path, device)\n",
    "  helpers.test_accuracy(model, X_test,y_test,video_indices, device)"
   ],
   "id": "-oZ6yVkTiDLa"
  },
  {
   "cell_type": "code",
   "source": [
    "helpers.test_accuracy(model, X_test,y_test,video_indices, device)"
   ],
   "metadata": {
    "id": "XBd09ApeR6gK"
   },
   "id": "XBd09ApeR6gK",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "JL877nSWZQZr"
   },
   "id": "JL877nSWZQZr",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
