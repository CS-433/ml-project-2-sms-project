{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "sorted-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "yellow-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fabulous-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "exercises_dict = {\n",
    "    'Abduction': 0,\n",
    "    'Bird': 1,\n",
    "    'Bridge': 2,\n",
    "    'Knee': 3,\n",
    "    'Shoulder': 4,\n",
    "    'Squat' : 5,\n",
    "    'Stretch' : 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "operational-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('prep_dataset.parquet', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attached-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c66bc7e-964b-4cb2-b1f3-0c675a0de69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices = df.index\n",
    "#labels = df.columns\n",
    "\n",
    "X = df.drop(['Participant', 'Exercise', 'Set', 'Camera'], axis=1)\n",
    "y_exercise = df['Exercise']\n",
    "#y_set = df['Set']\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2c5fccb-1f75-4350-bcbb-c666c5843e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_exercise, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(np.array([exercises_dict[y] for y in y_train.values]), dtype=torch.int64)\n",
    "\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(np.array([exercises_dict[y] for y in y_test.values]), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "lined-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mlp = lambda: torch.nn.Sequential(\n",
    "\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(100, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 7),\n",
    ")\n",
    "\n",
    "get_cnn = lambda: torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(in_channels=4, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv1d(32, 64, 5, stride=2, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv1d(64, 64, 5, stride=1, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv1d(64, 128, 5, stride=2, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.AdaptiveAvgPool1d(1),\n",
    "    torch.nn.Conv1d(128, 7, 1),\n",
    "    torch.nn.Flatten(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "laden-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Return a modified dataset cls that can insert MNIST like images into larger\n",
    "frames with an option for random shifts, scrambling the images in a\n",
    "consistent way (using the same shuffling for all images) and adding random\n",
    "Gaussian noise (to the base data, noise is always the same for a given\n",
    "image).\n",
    "\"\"\"\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_train, y_train, transform=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'input': self.x_train[idx], 'label': self.y_train[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "suffering-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "custom_dataset = CustomDataset(X_train, y_train)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 64\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = get_mlp()\n",
    "model.train()\n",
    "model_path = 'MLP.path'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "armed-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0823\n",
      "Epoch [2/10], Loss: 0.0837\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-9d7993717a3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#def train_epoch()\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch in custom_dataloader:\n",
    "        inputs, labels = batch['input'], batch['label']\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {np.mean(losses):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'MLP.path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bf449-e2e3-4312-9f18-d3144c920425",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Make predictions on the test set\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = torch.argmax(predictions, dim=1)\n",
    "\n",
    "# Convert tensors to numpy arrays\n",
    "y_test_np = y_test.numpy()\n",
    "predicted_labels_np = predicted_labels.numpy()\n",
    "\n",
    "# Print accuracy\n",
    "accuracy = accuracy_score(y_test_np, predicted_labels_np)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_np, predicted_labels_np, labels=range(7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "expanded-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 400  # Number of input channels\n",
    "hidden_size = 64  # Size of the hidden layer\n",
    "output_size = 7  # Number of classes (0 to 6)\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "custom_dataset = CustomDataset(x_train, y_train)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 64\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = get_cnn()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "moved-brisbane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4, 552545)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.moveaxis(x_train,-1,0)\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch in custom_dataloader:\n",
    "        inputs, labels = batch['input'], batch['label']\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(loss)\n",
    "        losses.append(loss.item())\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {np.mean(losses):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
